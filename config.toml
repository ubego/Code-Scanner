# Code Scanner Configuration

# Checks are organized into groups, each with a file pattern and list of rules.
# Pattern uses glob syntax: "*.cpp, *.h" matches C++ files, "*" matches all files.

# C++/Qt specific checks
[[checks]]
pattern = "*.cpp, *.h, *.cxx, *.hpp"
checks = [
    "Check for any detectable errors and suggest code simplifications where possible.",
    "Check that iteration continues automatically until the final result, without requiring user prompts to proceed.",
    "Check that constexpr and compile-time programming techniques are applied where appropriate.",
    "Check that stack allocation is preferred over heap allocation whenever possible.",
    "Check that string literals are handled through QStringView variables.",
    "Check that string literals used multiple times are stored in named QStringView constants instead of being repeated.",
    "Check that comments provide meaningful context or rationale and avoid restating obvious code behavior.",
    "Check that functions are implemented in .cpp files rather than .h files.",
    "Check for opportunities to optimize algorithms and data structures.",
]

# General checks for all files
[[checks]]
pattern = "*"
checks = [
    "Check for unused files or dead code.",
]

# LM Studio connection settings (optional - these are the defaults)
# [llm]
# host = "localhost"
# port = 1234
# model = "openai/gpt-oss-20b"  # Leave commented to use default model
# timeout = 120
# context_limit = 37000  # Context window size in tokens

[llm]
# -----------------------------------------------------------------------------
# Option 1: Ollama (recommended for simplicity)
# -----------------------------------------------------------------------------
# Install: https://ollama.ai/download
# Pull a model: ollama pull qwen3:4b
# Start server: ollama serve (runs automatically on macOS/Linux)

backend = "ollama"
host = "localhost"
port = 11434
model = "qwen3:4b"  # Required for Ollama
timeout = 120
# Optional: Override model's default context window
# Only set this if you know the model supports it
context_limit = 8192